services:
  sammyai:
    build: .
    container_name: sammyai_container
    environment:
      - DISPLAY=${DISPLAY}
      - QT_X11_NO_MITSHM=1
      - QT_DEBUG_PLUGINS=1
      - XAUTHORITY=/tmp/.Xauthority
    volumes:
      - /tmp/.X11-unix:/tmp/.X11-unix
      - ${XAUTHORITY:-$HOME/.Xauthority}:/tmp/.Xauthority
      - ./models:/app/models      # Persist downloaded HF/SentenceTransformers models
      - ./cache:/app/cache        # Persist ChromaDB index and embedding cache
    # If using local Ollama on host, you might need:
    # extra_hosts:
    #   - "host.docker.internal:host-gateway"
    network_mode: "host" # Use host networking to easily access local xserver and ollama
    stdin_open: true
    tty: true
